{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1 一些琐碎代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 RNNCell"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output size: torch.Size([1, 2])\n",
      "tensor([[-0.4140,  0.1517]], grad_fn=<TanhBackward0>)\n",
      "==================== 1 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output size: torch.Size([1, 2])\n",
      "tensor([[-0.4725, -0.7875]], grad_fn=<TanhBackward0>)\n",
      "==================== 2 ====================\n",
      "Input size: torch.Size([1, 4])\n",
      "output size: torch.Size([1, 2])\n",
      "tensor([[-0.8257, -0.2262]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "\n",
    "# Construction of RNNCell\n",
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "# Wrapping the sequence into:(seqLen,batchSize,InputSize)\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)  # (3,1,4)\n",
    "# Initializing the hidden to zero\n",
    "hidden = torch.zeros(batch_size, hidden_size)  # (1,2)\n",
    "\n",
    "for idx, input in enumerate(dataset):\n",
    "    print('=' * 20, idx, '=' * 20)  #分割线，20个=号\n",
    "    print('Input size:', input.shape)  # (batch_size, input_size)\n",
    "    # 按序列依次输入到cell中，seq_len=3，故循环3次\n",
    "    hidden = cell(input, hidden)  # 返回的hidden是下一次的输入之一，循环使用同一个cell\n",
    "\n",
    "    print('output size:', hidden.shape)  # (batch_size, hidden_size)\n",
    "    print(hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([3, 1, 2])\n",
      "Output: tensor([[[-0.9880, -0.8818]],\n",
      "\n",
      "        [[ 0.6066,  0.9090]],\n",
      "\n",
      "        [[-0.3108,  0.7957]]], grad_fn=<StackBackward0>)\n",
      "Hidden size: torch.Size([1, 1, 2])\n",
      "Hidden: tensor([[[-0.3108,  0.7957]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1  # RNN层数\n",
    "\n",
    "# Construction of RNN\n",
    "rnn = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "# Wrapping the sequence into:(seqLen,batchSize,InputSize)\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)  # (3,1,4)\n",
    "# Initializing the hidden to zero\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)  # (1,1,2)\n",
    "\n",
    "output, hidden = rnn(inputs, hidden)  # RNN内部包含了循环，故这里只需把整个序列输入即可\n",
    "\n",
    "print('Output size:', output.shape)  # (seq_len, batch_size, hidden_size)\n",
    "print('Output:', output)\n",
    "print('Hidden size:', hidden.shape)  # (num_layers, batch_size, hidden_size)\n",
    "print('Hidden:', hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 RNN参数：batch_first"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([1, 3, 2])\n",
      "Output: tensor([[[ 0.6276, -0.1454],\n",
      "         [ 0.0294,  0.3148],\n",
      "         [-0.3239,  0.4692]]], grad_fn=<TransposeBackward1>)\n",
      "Hidden size: torch.Size([1, 1, 2])\n",
      "Hidden: tensor([[[-0.3239,  0.4692]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1  # RNN层数\n",
    "\n",
    "# Construction of RNN, batch_first=True\n",
    "rnn = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "# 仅这里做了更改 Wrapping the sequence into:(batchSize,seqLen,InputSize)\n",
    "inputs = torch.randn(batch_size, seq_len, input_size)  # (1,3,4)\n",
    "# Initializing the hidden to zero\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)  # (1,1,2)\n",
    "\n",
    "output, hidden = rnn(inputs, hidden)  # RNN内部包含了循环，故这里只需把整个序列输入即可\n",
    "\n",
    "print('Output size:', output.shape)  # 仅输出维度发生变化(batch_size, seq_len, hidden_size)\n",
    "print('Output:', output)\n",
    "print('Hidden size:', hidden.shape)  # (num_layers, batch_size, hidden_size)\n",
    "print('Hidden:', hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 例子：序列变换把 \"hello\" --> \"ohlol\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 使用RNNCell"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:hehee, Epoch [1/15] loss: 8.2711\n",
      "Predicted string:olhll, Epoch [2/15] loss: 6.2931\n",
      "Predicted string:ollll, Epoch [3/15] loss: 5.3395\n",
      "Predicted string:ollll, Epoch [4/15] loss: 4.7223\n",
      "Predicted string:ohlll, Epoch [5/15] loss: 4.2614\n",
      "Predicted string:ohlll, Epoch [6/15] loss: 3.9137\n",
      "Predicted string:ohlol, Epoch [7/15] loss: 3.6579\n",
      "Predicted string:ohlol, Epoch [8/15] loss: 3.4601\n",
      "Predicted string:ohlol, Epoch [9/15] loss: 3.2896\n",
      "Predicted string:ohlol, Epoch [10/15] loss: 3.1306\n",
      "Predicted string:ohlol, Epoch [11/15] loss: 2.9806\n",
      "Predicted string:ohlol, Epoch [12/15] loss: 2.8476\n",
      "Predicted string:ohlol, Epoch [13/15] loss: 2.7450\n",
      "Predicted string:ohlol, Epoch [14/15] loss: 2.6792\n",
      "Predicted string:ohlol, Epoch [15/15] loss: 2.6347\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1、确定参数\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "\n",
    "# 2、准备数据\n",
    "index2char = ['e', 'h', 'l', 'o']  #字典\n",
    "x_data = [1, 0, 2, 2, 3]  #用字典中的索引（数字）表示来表示hello\n",
    "y_data = [3, 1, 2, 3, 2]  #标签：ohlol\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],  # 用来将x_data转换为one-hot向量的参照表\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]  #将x_data转换为one-hot向量\n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)  #(𝒔𝒆𝒒𝑳𝒆𝒏,𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆,𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆)\n",
    "labels = torch.LongTensor(y_data).view(-1, 1)  # (𝒔𝒆𝒒𝑳𝒆𝒏,𝟏).计算交叉熵损失时标签不需要我们进行one-hot编码，其内部会自动进行处理\n",
    "\n",
    "\n",
    "# 3、构建模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnncell = torch.nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnncell(input, hidden)\n",
    "        return hidden\n",
    "\n",
    "    def init_hidden(self):  #初始化隐藏层，需要batch_size\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)\n",
    "\n",
    "# 4、损失和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)  # Adam优化器\n",
    "\n",
    "# 5、训练\n",
    "for epoch in range(15):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()  #梯度清零\n",
    "    hidden = net.init_hidden()  # 初始化隐藏层\n",
    "    print('Predicted string:', end='')\n",
    "    for input, label in zip(inputs, labels):  #每次输入一个字符，即按序列次序进行循环\n",
    "        hidden = net(input, hidden)\n",
    "        loss += criterion(hidden, label)  # 计算损失，不用item()，因为后面还要反向传播\n",
    "        _, idx = hidden.max(dim=1)  # 选取最大值的索引\n",
    "        print(index2char[idx.item()], end='')  # 打印预测的字符\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    print(', Epoch [%d/15] loss: %.4f' % (epoch + 1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 使用RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  hhhhh, Epoch [1/15] loss: 1.4325\n",
      "Predicted string:  hhhhh, Epoch [2/15] loss: 1.2532\n",
      "Predicted string:  ohhoh, Epoch [3/15] loss: 1.1057\n",
      "Predicted string:  ohlol, Epoch [4/15] loss: 0.9970\n",
      "Predicted string:  ohlol, Epoch [5/15] loss: 0.9208\n",
      "Predicted string:  oolol, Epoch [6/15] loss: 0.8669\n",
      "Predicted string:  oolol, Epoch [7/15] loss: 0.8250\n",
      "Predicted string:  oolol, Epoch [8/15] loss: 0.7863\n",
      "Predicted string:  oolol, Epoch [9/15] loss: 0.7453\n",
      "Predicted string:  oolol, Epoch [10/15] loss: 0.7024\n",
      "Predicted string:  oolol, Epoch [11/15] loss: 0.6625\n",
      "Predicted string:  oolol, Epoch [12/15] loss: 0.6291\n",
      "Predicted string:  ohlol, Epoch [13/15] loss: 0.6026\n",
      "Predicted string:  ohlol, Epoch [14/15] loss: 0.5812\n",
      "Predicted string:  ohlol, Epoch [15/15] loss: 0.5630\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1、确定参数\n",
    "seq_len = 5\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "\n",
    "# 2、准备数据\n",
    "index2char = ['e', 'h', 'l', 'o']  #字典\n",
    "x_data = [1, 0, 2, 2, 3]  #用字典中的索引（数字）表示来表示hello\n",
    "y_data = [3, 1, 2, 3, 2]  #标签：ohlol\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],  # 用来将x_data转换为one-hot向量的参照表\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]  #将x_data转换为one-hot向量\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size,\n",
    "                                      input_size)  #(𝒔𝒆𝒒𝑳𝒆𝒏,𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆,𝒊𝒏𝒑𝒖𝒕𝑺𝒊𝒛𝒆)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "\n",
    "# 3、构建模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = torch.nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "        out, _ = self.rnn(input, hidden)  # out: tensor of shape (seq_len, batch, hidden_size)\n",
    "        return out.view(-1, self.hidden_size)  # 将输出的三维张量转换为二维张量,(𝒔𝒆𝒒𝑳𝒆𝒏×𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆,𝒉𝒊𝒅𝒅𝒆𝒏𝑺𝒊𝒛𝒆)\n",
    "\n",
    "    def init_hidden(self):  #初始化隐藏层，需要batch_size\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size, num_layers)\n",
    "\n",
    "# 4、损失和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)  # Adam优化器\n",
    "\n",
    "# 5、训练\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ', ''.join([index2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/15] loss: %.4f' % (epoch + 1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 使用embedding and linear layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  eeeee, Epoch [1/15] loss: 1.5407\n",
      "Predicted string:  oolol, Epoch [2/15] loss: 1.1158\n",
      "Predicted string:  oolol, Epoch [3/15] loss: 0.9047\n",
      "Predicted string:  ohlol, Epoch [4/15] loss: 0.7391\n",
      "Predicted string:  lhlol, Epoch [5/15] loss: 0.6006\n",
      "Predicted string:  ohlol, Epoch [6/15] loss: 0.4833\n",
      "Predicted string:  ohlol, Epoch [7/15] loss: 0.3581\n",
      "Predicted string:  ohlol, Epoch [8/15] loss: 0.2540\n",
      "Predicted string:  ohlol, Epoch [9/15] loss: 0.1921\n",
      "Predicted string:  ohlol, Epoch [10/15] loss: 0.1351\n",
      "Predicted string:  ohlol, Epoch [11/15] loss: 0.0972\n",
      "Predicted string:  ohlol, Epoch [12/15] loss: 0.0752\n",
      "Predicted string:  ohlol, Epoch [13/15] loss: 0.0594\n",
      "Predicted string:  ohlol, Epoch [14/15] loss: 0.0465\n",
      "Predicted string:  ohlol, Epoch [15/15] loss: 0.0363\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1、确定参数\n",
    "num_class = 4\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "embedding_size = 10\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "# 2、准备数据\n",
    "index2char = ['e', 'h', 'l', 'o']  #字典\n",
    "x_data = [[1, 0, 2, 2, 3]]  # (batch_size, seq_len) 用字典中的索引（数字）表示来表示hello\n",
    "y_data = [3, 1, 2, 3, 2]  #  (batch_size * seq_len) 标签：ohlol\n",
    "\n",
    "inputs = torch.LongTensor(x_data)  # (batch_size, seq_len)\n",
    "labels = torch.LongTensor(y_data)  # (batch_size * seq_len)\n",
    "\n",
    "\n",
    "# 3、构建模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(num_class, embedding_size)\n",
    "        self.rnn = torch.nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size)  # (num_layers, batch_size, hidden_size)\n",
    "        x = self.emb(x)  # 返回(batch_size, seq_len, embedding_size)\n",
    "        x, _ = self.rnn(x, hidden)  # 返回(batch_size, seq_len, hidden_size)\n",
    "        x = self.fc(x)  # 返回(batch_size, seq_len, num_class)\n",
    "        return x.view(-1, num_class)  # (batch_size * seq_len, num_class)\n",
    "\n",
    "\n",
    "net = Model()\n",
    "\n",
    "# 4、损失和优化器\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)  # Adam优化器\n",
    "\n",
    "# 5、训练\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ', ''.join([index2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/15] loss: %.4f' % (epoch + 1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "这一章的代码比较逆天，具体看注释。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs...\n",
      "[0min 2s] Epoch 1 [2560/13374] loss=0.00880123688839376\n",
      "[0min 4s] Epoch 1 [5120/13374] loss=0.007671719812788069\n",
      "[0min 6s] Epoch 1 [7680/13374] loss=0.007059914680818717\n",
      "[0min 8s] Epoch 1 [10240/13374] loss=0.006611915945541114\n",
      "[0min 11s] Epoch 1 [12800/13374] loss=0.006199482078664005\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4404/6700 65.73%\n",
      "[0min 14s] Epoch 2 [2560/13374] loss=0.004212932661175728\n",
      "[0min 17s] Epoch 2 [5120/13374] loss=0.0041206718306057155\n",
      "[0min 19s] Epoch 2 [7680/13374] loss=0.004030837615331014\n",
      "[0min 21s] Epoch 2 [10240/13374] loss=0.003940029616933316\n",
      "[0min 23s] Epoch 2 [12800/13374] loss=0.0038458853447809814\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 4946/6700 73.82%\n",
      "[0min 27s] Epoch 3 [2560/13374] loss=0.003235860029235482\n",
      "[0min 29s] Epoch 3 [5120/13374] loss=0.0030549810151569547\n",
      "[0min 32s] Epoch 3 [7680/13374] loss=0.0030818649490053454\n",
      "[0min 34s] Epoch 3 [10240/13374] loss=0.0030392331187613308\n",
      "[0min 36s] Epoch 3 [12800/13374] loss=0.003027400164864957\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5211/6700 77.78%\n",
      "[0min 40s] Epoch 4 [2560/13374] loss=0.0025919046718627216\n",
      "[0min 42s] Epoch 4 [5120/13374] loss=0.002566435094922781\n",
      "[0min 45s] Epoch 4 [7680/13374] loss=0.0025535416013250747\n",
      "[0min 47s] Epoch 4 [10240/13374] loss=0.0025712428090628237\n",
      "[0min 49s] Epoch 4 [12800/13374] loss=0.0025416440609842537\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5379/6700 80.28%\n",
      "[0min 52s] Epoch 5 [2560/13374] loss=0.002134323725476861\n",
      "[0min 54s] Epoch 5 [5120/13374] loss=0.002163298265077174\n",
      "[0min 56s] Epoch 5 [7680/13374] loss=0.0021756061702035367\n",
      "[0min 59s] Epoch 5 [10240/13374] loss=0.002216084487736225\n",
      "[1min 1s] Epoch 5 [12800/13374] loss=0.002211462967097759\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5436/6700 81.13%\n",
      "[1min 4s] Epoch 6 [2560/13374] loss=0.0019523774622939526\n",
      "[1min 6s] Epoch 6 [5120/13374] loss=0.001996276725549251\n",
      "[1min 9s] Epoch 6 [7680/13374] loss=0.0020501446289320786\n",
      "[1min 11s] Epoch 6 [10240/13374] loss=0.0020126879273448138\n",
      "[1min 13s] Epoch 6 [12800/13374] loss=0.0020067573175765576\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5517/6700 82.34%\n",
      "[1min 17s] Epoch 7 [2560/13374] loss=0.0017880295985378326\n",
      "[1min 18s] Epoch 7 [5120/13374] loss=0.0018156664504203946\n",
      "[1min 20s] Epoch 7 [7680/13374] loss=0.0017988201230764388\n",
      "[1min 22s] Epoch 7 [10240/13374] loss=0.0018154591118218378\n",
      "[1min 24s] Epoch 7 [12800/13374] loss=0.0018141932366415859\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5570/6700 83.13%\n",
      "[1min 28s] Epoch 8 [2560/13374] loss=0.0016374781494960188\n",
      "[1min 30s] Epoch 8 [5120/13374] loss=0.001647920353570953\n",
      "[1min 32s] Epoch 8 [7680/13374] loss=0.0016479324083775281\n",
      "[1min 34s] Epoch 8 [10240/13374] loss=0.0016383340727770701\n",
      "[1min 36s] Epoch 8 [12800/13374] loss=0.0016434452240355313\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5589/6700 83.42%\n",
      "[1min 40s] Epoch 9 [2560/13374] loss=0.0015151496743783354\n",
      "[1min 42s] Epoch 9 [5120/13374] loss=0.0015128103841561824\n",
      "[1min 44s] Epoch 9 [7680/13374] loss=0.0014915980864316225\n",
      "[1min 46s] Epoch 9 [10240/13374] loss=0.0014710453629959374\n",
      "[1min 48s] Epoch 9 [12800/13374] loss=0.001470922315493226\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5600/6700 83.58%\n",
      "[1min 52s] Epoch 10 [2560/13374] loss=0.0013561381492763758\n",
      "[1min 54s] Epoch 10 [5120/13374] loss=0.0013016103068366647\n",
      "[1min 56s] Epoch 10 [7680/13374] loss=0.0013004067237488925\n",
      "[1min 58s] Epoch 10 [10240/13374] loss=0.0013108428451232613\n",
      "[2min 0s] Epoch 10 [12800/13374] loss=0.0013268653419800103\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[2min 3s] Epoch 11 [2560/13374] loss=0.0011720586102455855\n",
      "[2min 5s] Epoch 11 [5120/13374] loss=0.0011937652830965816\n",
      "[2min 7s] Epoch 11 [7680/13374] loss=0.001204646590243404\n",
      "[2min 9s] Epoch 11 [10240/13374] loss=0.0012167622146080248\n",
      "[2min 11s] Epoch 11 [12800/13374] loss=0.0012066927901469172\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5657/6700 84.43%\n",
      "[2min 15s] Epoch 12 [2560/13374] loss=0.001126813457813114\n",
      "[2min 17s] Epoch 12 [5120/13374] loss=0.001106326631270349\n",
      "[2min 19s] Epoch 12 [7680/13374] loss=0.001104128339405482\n",
      "[2min 21s] Epoch 12 [10240/13374] loss=0.0010923102730885149\n",
      "[2min 23s] Epoch 12 [12800/13374] loss=0.0010878108604811133\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5671/6700 84.64%\n",
      "[2min 27s] Epoch 13 [2560/13374] loss=0.0009443809976801276\n",
      "[2min 29s] Epoch 13 [5120/13374] loss=0.0009443133982131258\n",
      "[2min 31s] Epoch 13 [7680/13374] loss=0.0009449393119818221\n",
      "[2min 34s] Epoch 13 [10240/13374] loss=0.0009550946415401995\n",
      "[2min 36s] Epoch 13 [12800/13374] loss=0.000974101530155167\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5646/6700 84.27%\n",
      "[2min 40s] Epoch 14 [2560/13374] loss=0.0008212433313019574\n",
      "[2min 42s] Epoch 14 [5120/13374] loss=0.000847443233942613\n",
      "[2min 44s] Epoch 14 [7680/13374] loss=0.0008504848772039016\n",
      "[2min 46s] Epoch 14 [10240/13374] loss=0.0008589115139329806\n",
      "[2min 48s] Epoch 14 [12800/13374] loss=0.0008747856679838151\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5648/6700 84.30%\n",
      "[2min 52s] Epoch 15 [2560/13374] loss=0.000731211673701182\n",
      "[2min 54s] Epoch 15 [5120/13374] loss=0.0007119103858713061\n",
      "[2min 56s] Epoch 15 [7680/13374] loss=0.0007274391983325283\n",
      "[2min 58s] Epoch 15 [10240/13374] loss=0.0007321385404793545\n",
      "[3min 0s] Epoch 15 [12800/13374] loss=0.0007505938655231148\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5639/6700 84.16%\n",
      "[3min 4s] Epoch 16 [2560/13374] loss=0.0007051935186609626\n",
      "[3min 6s] Epoch 16 [5120/13374] loss=0.0006788263039197773\n",
      "[3min 8s] Epoch 16 [7680/13374] loss=0.0006818203236131618\n",
      "[3min 10s] Epoch 16 [10240/13374] loss=0.0006767702790966724\n",
      "[3min 12s] Epoch 16 [12800/13374] loss=0.0006846041971584782\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[3min 16s] Epoch 17 [2560/13374] loss=0.000553301622858271\n",
      "[3min 18s] Epoch 17 [5120/13374] loss=0.0005497235950315371\n",
      "[3min 20s] Epoch 17 [7680/13374] loss=0.0005588171809601287\n",
      "[3min 22s] Epoch 17 [10240/13374] loss=0.0005839110628585332\n",
      "[3min 24s] Epoch 17 [12800/13374] loss=0.0005981582414824515\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5635/6700 84.10%\n",
      "[3min 27s] Epoch 18 [2560/13374] loss=0.0005055507470387965\n",
      "[3min 29s] Epoch 18 [5120/13374] loss=0.0005080900315078907\n",
      "[3min 31s] Epoch 18 [7680/13374] loss=0.0005279592849547043\n",
      "[3min 33s] Epoch 18 [10240/13374] loss=0.0005251290334854275\n",
      "[3min 35s] Epoch 18 [12800/13374] loss=0.0005300983460620046\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5640/6700 84.18%\n",
      "[3min 39s] Epoch 19 [2560/13374] loss=0.00042675461154431107\n",
      "[3min 41s] Epoch 19 [5120/13374] loss=0.0004581703688018024\n",
      "[3min 43s] Epoch 19 [7680/13374] loss=0.00046874297161897025\n",
      "[3min 45s] Epoch 19 [10240/13374] loss=0.0004826035517908167\n",
      "[3min 47s] Epoch 19 [12800/13374] loss=0.0004893425811314955\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5630/6700 84.03%\n",
      "[3min 51s] Epoch 20 [2560/13374] loss=0.0004173647321294993\n",
      "[3min 53s] Epoch 20 [5120/13374] loss=0.00042232975974911826\n",
      "[3min 55s] Epoch 20 [7680/13374] loss=0.0004248344184209903\n",
      "[3min 57s] Epoch 20 [10240/13374] loss=0.0004286278621293604\n",
      "[3min 59s] Epoch 20 [12800/13374] loss=0.0004421587596880272\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5616/6700 83.82%\n",
      "[4min 2s] Epoch 21 [2560/13374] loss=0.00036439104296732695\n",
      "[4min 4s] Epoch 21 [5120/13374] loss=0.00037994637386873364\n",
      "[4min 6s] Epoch 21 [7680/13374] loss=0.00039206515066325665\n",
      "[4min 8s] Epoch 21 [10240/13374] loss=0.0003868610361678293\n",
      "[4min 10s] Epoch 21 [12800/13374] loss=0.00039005800761515274\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5630/6700 84.03%\n",
      "[4min 14s] Epoch 22 [2560/13374] loss=0.00034021106548607347\n",
      "[4min 16s] Epoch 22 [5120/13374] loss=0.00032582944113528355\n",
      "[4min 18s] Epoch 22 [7680/13374] loss=0.00034239477051111557\n",
      "[4min 20s] Epoch 22 [10240/13374] loss=0.0003435979328060057\n",
      "[4min 22s] Epoch 22 [12800/13374] loss=0.0003618116752477363\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[4min 26s] Epoch 23 [2560/13374] loss=0.0003265024584834464\n",
      "[4min 28s] Epoch 23 [5120/13374] loss=0.00032626209140289574\n",
      "[4min 31s] Epoch 23 [7680/13374] loss=0.00031661731045460326\n",
      "[4min 33s] Epoch 23 [10240/13374] loss=0.0003175979694788111\n",
      "[4min 35s] Epoch 23 [12800/13374] loss=0.00033259443647693844\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[4min 39s] Epoch 24 [2560/13374] loss=0.00026290965033695104\n",
      "[4min 41s] Epoch 24 [5120/13374] loss=0.0003189852446666919\n",
      "[4min 43s] Epoch 24 [7680/13374] loss=0.00032448422571178525\n",
      "[4min 46s] Epoch 24 [10240/13374] loss=0.0003171241765812738\n",
      "[4min 48s] Epoch 24 [12800/13374] loss=0.00031823426397750156\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[4min 52s] Epoch 25 [2560/13374] loss=0.0002473006388754584\n",
      "[4min 54s] Epoch 25 [5120/13374] loss=0.0002556614352215547\n",
      "[4min 56s] Epoch 25 [7680/13374] loss=0.0002722034749846595\n",
      "[4min 58s] Epoch 25 [10240/13374] loss=0.00027576357933867257\n",
      "[5min 0s] Epoch 25 [12800/13374] loss=0.00029420699487673116\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5594/6700 83.49%\n",
      "[5min 4s] Epoch 26 [2560/13374] loss=0.00025600041408324614\n",
      "[5min 6s] Epoch 26 [5120/13374] loss=0.0002699460623261984\n",
      "[5min 8s] Epoch 26 [7680/13374] loss=0.0002689897461095825\n",
      "[5min 10s] Epoch 26 [10240/13374] loss=0.0002896908950788202\n",
      "[5min 12s] Epoch 26 [12800/13374] loss=0.00029467555519659073\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[5min 16s] Epoch 27 [2560/13374] loss=0.00021719656360801308\n",
      "[5min 19s] Epoch 27 [5120/13374] loss=0.00024387663361267187\n",
      "[5min 21s] Epoch 27 [7680/13374] loss=0.0002453492187972491\n",
      "[5min 23s] Epoch 27 [10240/13374] loss=0.00025968431145884097\n",
      "[5min 25s] Epoch 27 [12800/13374] loss=0.00026680457667680455\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[5min 29s] Epoch 28 [2560/13374] loss=0.0002078653567878064\n",
      "[5min 31s] Epoch 28 [5120/13374] loss=0.00022717175779689568\n",
      "[5min 33s] Epoch 28 [7680/13374] loss=0.00023232178088316384\n",
      "[5min 35s] Epoch 28 [10240/13374] loss=0.00024382786887144903\n",
      "[5min 37s] Epoch 28 [12800/13374] loss=0.00025318116488051603\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5596/6700 83.52%\n",
      "[5min 41s] Epoch 29 [2560/13374] loss=0.0002579605745268054\n",
      "[5min 43s] Epoch 29 [5120/13374] loss=0.000257213046279503\n",
      "[5min 45s] Epoch 29 [7680/13374] loss=0.0002456716678959007\n",
      "[5min 47s] Epoch 29 [10240/13374] loss=0.00026151088713959324\n",
      "[5min 49s] Epoch 29 [12800/13374] loss=0.0002619160761241801\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5593/6700 83.48%\n",
      "[5min 52s] Epoch 30 [2560/13374] loss=0.0001754869670548942\n",
      "[5min 54s] Epoch 30 [5120/13374] loss=0.00019981456898676697\n",
      "[5min 56s] Epoch 30 [7680/13374] loss=0.000217827852126599\n",
      "[5min 58s] Epoch 30 [10240/13374] loss=0.00022728788298991275\n",
      "[6min 0s] Epoch 30 [12800/13374] loss=0.0002370569262711797\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5589/6700 83.42%\n",
      "[6min 4s] Epoch 31 [2560/13374] loss=0.00020217589189996943\n",
      "[6min 6s] Epoch 31 [5120/13374] loss=0.00018988049851031974\n",
      "[6min 8s] Epoch 31 [7680/13374] loss=0.0002076508705310213\n",
      "[6min 10s] Epoch 31 [10240/13374] loss=0.0002212729727034457\n",
      "[6min 12s] Epoch 31 [12800/13374] loss=0.00022750262753106653\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[6min 15s] Epoch 32 [2560/13374] loss=0.00019298958650324494\n",
      "[6min 17s] Epoch 32 [5120/13374] loss=0.00020393144222907722\n",
      "[6min 19s] Epoch 32 [7680/13374] loss=0.0002069301723774212\n",
      "[6min 21s] Epoch 32 [10240/13374] loss=0.0002291890534252161\n",
      "[6min 23s] Epoch 32 [12800/13374] loss=0.00023240015900228173\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5586/6700 83.37%\n",
      "[6min 27s] Epoch 33 [2560/13374] loss=0.00018146126094507054\n",
      "[6min 29s] Epoch 33 [5120/13374] loss=0.00018636008753674104\n",
      "[6min 31s] Epoch 33 [7680/13374] loss=0.00019561274093575775\n",
      "[6min 33s] Epoch 33 [10240/13374] loss=0.00020797457364096772\n",
      "[6min 36s] Epoch 33 [12800/13374] loss=0.00022133371574454942\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[6min 40s] Epoch 34 [2560/13374] loss=0.0001873428249382414\n",
      "[6min 42s] Epoch 34 [5120/13374] loss=0.00019913285577786156\n",
      "[6min 44s] Epoch 34 [7680/13374] loss=0.0002028888869972434\n",
      "[6min 46s] Epoch 34 [10240/13374] loss=0.00020615073499357094\n",
      "[6min 48s] Epoch 34 [12800/13374] loss=0.00022122286914964207\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[6min 51s] Epoch 35 [2560/13374] loss=0.0001446299225790426\n",
      "[6min 53s] Epoch 35 [5120/13374] loss=0.00017170179125969298\n",
      "[6min 55s] Epoch 35 [7680/13374] loss=0.00017984229101178546\n",
      "[6min 57s] Epoch 35 [10240/13374] loss=0.0001974914990569232\n",
      "[6min 59s] Epoch 35 [12800/13374] loss=0.00020963591552572325\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5632/6700 84.06%\n",
      "[7min 3s] Epoch 36 [2560/13374] loss=0.00020015126283396968\n",
      "[7min 5s] Epoch 36 [5120/13374] loss=0.00020028290200571063\n",
      "[7min 7s] Epoch 36 [7680/13374] loss=0.00021156838120077736\n",
      "[7min 10s] Epoch 36 [10240/13374] loss=0.00021338533497328172\n",
      "[7min 12s] Epoch 36 [12800/13374] loss=0.00021970703746774234\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[7min 16s] Epoch 37 [2560/13374] loss=0.0001438406965462491\n",
      "[7min 18s] Epoch 37 [5120/13374] loss=0.0001763289503287524\n",
      "[7min 20s] Epoch 37 [7680/13374] loss=0.00018427161024495338\n",
      "[7min 22s] Epoch 37 [10240/13374] loss=0.00019444797144387848\n",
      "[7min 25s] Epoch 37 [12800/13374] loss=0.00020997127023292706\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[7min 28s] Epoch 38 [2560/13374] loss=0.00016800117155071347\n",
      "[7min 31s] Epoch 38 [5120/13374] loss=0.00017900418824865483\n",
      "[7min 33s] Epoch 38 [7680/13374] loss=0.00019654276644966254\n",
      "[7min 35s] Epoch 38 [10240/13374] loss=0.0002053915934084216\n",
      "[7min 37s] Epoch 38 [12800/13374] loss=0.00020664825715357437\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[7min 41s] Epoch 39 [2560/13374] loss=0.00015958416988723912\n",
      "[7min 43s] Epoch 39 [5120/13374] loss=0.0001574696696479805\n",
      "[7min 45s] Epoch 39 [7680/13374] loss=0.00017933873265671234\n",
      "[7min 47s] Epoch 39 [10240/13374] loss=0.00019481591043586378\n",
      "[7min 49s] Epoch 39 [12800/13374] loss=0.0001952573507151101\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[7min 53s] Epoch 40 [2560/13374] loss=0.00015667003535781986\n",
      "[7min 55s] Epoch 40 [5120/13374] loss=0.00015361881196440663\n",
      "[7min 58s] Epoch 40 [7680/13374] loss=0.00016233824620333812\n",
      "[8min 0s] Epoch 40 [10240/13374] loss=0.00017473979423812124\n",
      "[8min 2s] Epoch 40 [12800/13374] loss=0.00019761423813179134\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[8min 6s] Epoch 41 [2560/13374] loss=0.000206899295153562\n",
      "[8min 9s] Epoch 41 [5120/13374] loss=0.00018897283371188678\n",
      "[8min 11s] Epoch 41 [7680/13374] loss=0.00019355774275027217\n",
      "[8min 13s] Epoch 41 [10240/13374] loss=0.00019244947470724582\n",
      "[8min 15s] Epoch 41 [12800/13374] loss=0.00019914649601560086\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5603/6700 83.63%\n",
      "[8min 18s] Epoch 42 [2560/13374] loss=0.0001548045613162685\n",
      "[8min 20s] Epoch 42 [5120/13374] loss=0.0001623240332264686\n",
      "[8min 23s] Epoch 42 [7680/13374] loss=0.00018504550122694735\n",
      "[8min 25s] Epoch 42 [10240/13374] loss=0.00019938892201025736\n",
      "[8min 27s] Epoch 42 [12800/13374] loss=0.00020329327540821395\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[8min 31s] Epoch 43 [2560/13374] loss=0.00015642291255062445\n",
      "[8min 33s] Epoch 43 [5120/13374] loss=0.00015637429496564436\n",
      "[8min 35s] Epoch 43 [7680/13374] loss=0.00017574661809097354\n",
      "[8min 37s] Epoch 43 [10240/13374] loss=0.00017946784373634728\n",
      "[8min 39s] Epoch 43 [12800/13374] loss=0.00019590486866945866\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[8min 43s] Epoch 44 [2560/13374] loss=0.00013671223059645853\n",
      "[8min 45s] Epoch 44 [5120/13374] loss=0.00015465516516997014\n",
      "[8min 47s] Epoch 44 [7680/13374] loss=0.0001619963368284516\n",
      "[8min 49s] Epoch 44 [10240/13374] loss=0.00018020573224930558\n",
      "[8min 51s] Epoch 44 [12800/13374] loss=0.0001913371609407477\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[8min 55s] Epoch 45 [2560/13374] loss=0.00013595318741863594\n",
      "[8min 57s] Epoch 45 [5120/13374] loss=0.00017108411630033514\n",
      "[8min 59s] Epoch 45 [7680/13374] loss=0.0001796066586393863\n",
      "[9min 1s] Epoch 45 [10240/13374] loss=0.00018322906107641755\n",
      "[9min 3s] Epoch 45 [12800/13374] loss=0.00019003951456397772\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[9min 6s] Epoch 46 [2560/13374] loss=0.00014755406591575592\n",
      "[9min 8s] Epoch 46 [5120/13374] loss=0.0001644150968786562\n",
      "[9min 10s] Epoch 46 [7680/13374] loss=0.00017831451186793855\n",
      "[9min 12s] Epoch 46 [10240/13374] loss=0.00018575370104372267\n",
      "[9min 14s] Epoch 46 [12800/13374] loss=0.00018347430042922497\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5587/6700 83.39%\n",
      "[9min 18s] Epoch 47 [2560/13374] loss=0.00014888605946907773\n",
      "[9min 20s] Epoch 47 [5120/13374] loss=0.0001520652702311054\n",
      "[9min 22s] Epoch 47 [7680/13374] loss=0.00016665884492491994\n",
      "[9min 24s] Epoch 47 [10240/13374] loss=0.00018013225653703557\n",
      "[9min 26s] Epoch 47 [12800/13374] loss=0.00018522493410273456\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[9min 29s] Epoch 48 [2560/13374] loss=0.00012796875635103788\n",
      "[9min 31s] Epoch 48 [5120/13374] loss=0.00016444166303699603\n",
      "[9min 33s] Epoch 48 [7680/13374] loss=0.00017060257462920466\n",
      "[9min 35s] Epoch 48 [10240/13374] loss=0.00017234451906915637\n",
      "[9min 37s] Epoch 48 [12800/13374] loss=0.00018909177735622505\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[9min 41s] Epoch 49 [2560/13374] loss=0.0001539528850116767\n",
      "[9min 43s] Epoch 49 [5120/13374] loss=0.00015579772953060455\n",
      "[9min 45s] Epoch 49 [7680/13374] loss=0.000157998353097355\n",
      "[9min 47s] Epoch 49 [10240/13374] loss=0.00017704074471112108\n",
      "[9min 49s] Epoch 49 [12800/13374] loss=0.00017895708704600112\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[9min 53s] Epoch 50 [2560/13374] loss=0.00015009088820079343\n",
      "[9min 54s] Epoch 50 [5120/13374] loss=0.00016008152379072272\n",
      "[9min 56s] Epoch 50 [7680/13374] loss=0.00016880075903221345\n",
      "[9min 58s] Epoch 50 [10240/13374] loss=0.00016790165718703066\n",
      "[10min 0s] Epoch 50 [12800/13374] loss=0.00017879922641441225\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5604/6700 83.64%\n",
      "[10min 4s] Epoch 51 [2560/13374] loss=0.00013948436753707938\n",
      "[10min 6s] Epoch 51 [5120/13374] loss=0.00015515781451540532\n",
      "[10min 8s] Epoch 51 [7680/13374] loss=0.0001658731976931449\n",
      "[10min 10s] Epoch 51 [10240/13374] loss=0.00017522523576189996\n",
      "[10min 12s] Epoch 51 [12800/13374] loss=0.00017938856122782455\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[10min 16s] Epoch 52 [2560/13374] loss=0.00012459093595680316\n",
      "[10min 17s] Epoch 52 [5120/13374] loss=0.00014628490953327855\n",
      "[10min 19s] Epoch 52 [7680/13374] loss=0.00016599391274212393\n",
      "[10min 21s] Epoch 52 [10240/13374] loss=0.0001684258236309688\n",
      "[10min 23s] Epoch 52 [12800/13374] loss=0.00017555687394633424\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[10min 27s] Epoch 53 [2560/13374] loss=0.0001390661156619899\n",
      "[10min 29s] Epoch 53 [5120/13374] loss=0.00015459505084436387\n",
      "[10min 31s] Epoch 53 [7680/13374] loss=0.00015630373939832983\n",
      "[10min 33s] Epoch 53 [10240/13374] loss=0.0001632556624826975\n",
      "[10min 35s] Epoch 53 [12800/13374] loss=0.00017307940142927692\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[10min 39s] Epoch 54 [2560/13374] loss=0.00013232695200713352\n",
      "[10min 40s] Epoch 54 [5120/13374] loss=0.0001421925706381444\n",
      "[10min 42s] Epoch 54 [7680/13374] loss=0.00014915957993556124\n",
      "[10min 44s] Epoch 54 [10240/13374] loss=0.00016247649964498123\n",
      "[10min 46s] Epoch 54 [12800/13374] loss=0.0001689072967565153\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5624/6700 83.94%\n",
      "[10min 50s] Epoch 55 [2560/13374] loss=0.0001427248032996431\n",
      "[10min 52s] Epoch 55 [5120/13374] loss=0.00013401120704656933\n",
      "[10min 54s] Epoch 55 [7680/13374] loss=0.00015732850127581817\n",
      "[10min 56s] Epoch 55 [10240/13374] loss=0.00016593263881077293\n",
      "[10min 58s] Epoch 55 [12800/13374] loss=0.00017328779096715153\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[11min 2s] Epoch 56 [2560/13374] loss=0.00011705851611623074\n",
      "[11min 4s] Epoch 56 [5120/13374] loss=0.00013572900952567578\n",
      "[11min 5s] Epoch 56 [7680/13374] loss=0.000149391758289615\n",
      "[11min 7s] Epoch 56 [10240/13374] loss=0.00016213889657592517\n",
      "[11min 9s] Epoch 56 [12800/13374] loss=0.00017025724540872033\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5638/6700 84.15%\n",
      "[11min 13s] Epoch 57 [2560/13374] loss=0.00015496100750169716\n",
      "[11min 15s] Epoch 57 [5120/13374] loss=0.00015713249958935194\n",
      "[11min 17s] Epoch 57 [7680/13374] loss=0.00016442715883992302\n",
      "[11min 19s] Epoch 57 [10240/13374] loss=0.00016693774232408033\n",
      "[11min 21s] Epoch 57 [12800/13374] loss=0.0001742738572647795\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5634/6700 84.09%\n",
      "[11min 25s] Epoch 58 [2560/13374] loss=0.000132038738229312\n",
      "[11min 27s] Epoch 58 [5120/13374] loss=0.000151387948426418\n",
      "[11min 29s] Epoch 58 [7680/13374] loss=0.000155538094380366\n",
      "[11min 30s] Epoch 58 [10240/13374] loss=0.00016318308398695082\n",
      "[11min 32s] Epoch 58 [12800/13374] loss=0.0001715309378778329\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[11min 36s] Epoch 59 [2560/13374] loss=0.00014678296065540053\n",
      "[11min 38s] Epoch 59 [5120/13374] loss=0.00013781023590127007\n",
      "[11min 40s] Epoch 59 [7680/13374] loss=0.00015260530344676226\n",
      "[11min 42s] Epoch 59 [10240/13374] loss=0.00015983087196218547\n",
      "[11min 44s] Epoch 59 [12800/13374] loss=0.00016998684950522147\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[11min 48s] Epoch 60 [2560/13374] loss=0.00015129180828807876\n",
      "[11min 49s] Epoch 60 [5120/13374] loss=0.000141998752951622\n",
      "[11min 51s] Epoch 60 [7680/13374] loss=0.00015949025122002542\n",
      "[11min 53s] Epoch 60 [10240/13374] loss=0.00016039843185353675\n",
      "[11min 55s] Epoch 60 [12800/13374] loss=0.00016740082115575207\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5626/6700 83.97%\n",
      "[11min 59s] Epoch 61 [2560/13374] loss=0.00011383718447177671\n",
      "[12min 1s] Epoch 61 [5120/13374] loss=0.00012083536785212345\n",
      "[12min 3s] Epoch 61 [7680/13374] loss=0.00013857494219943572\n",
      "[12min 5s] Epoch 61 [10240/13374] loss=0.00015866889680182794\n",
      "[12min 7s] Epoch 61 [12800/13374] loss=0.0001678186828212347\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[12min 10s] Epoch 62 [2560/13374] loss=0.00012540295356302522\n",
      "[12min 12s] Epoch 62 [5120/13374] loss=0.00013369454136409332\n",
      "[12min 14s] Epoch 62 [7680/13374] loss=0.00014042863598054585\n",
      "[12min 16s] Epoch 62 [10240/13374] loss=0.00015838587514735992\n",
      "[12min 18s] Epoch 62 [12800/13374] loss=0.00016512048619915731\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[12min 22s] Epoch 63 [2560/13374] loss=0.00014939370230422356\n",
      "[12min 24s] Epoch 63 [5120/13374] loss=0.00015539796768280212\n",
      "[12min 26s] Epoch 63 [7680/13374] loss=0.0001615710098121781\n",
      "[12min 28s] Epoch 63 [10240/13374] loss=0.0001575256756041199\n",
      "[12min 30s] Epoch 63 [12800/13374] loss=0.0001624982157954946\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[12min 33s] Epoch 64 [2560/13374] loss=0.00015041025617392733\n",
      "[12min 35s] Epoch 64 [5120/13374] loss=0.00014675773818453308\n",
      "[12min 37s] Epoch 64 [7680/13374] loss=0.00015876798449122967\n",
      "[12min 39s] Epoch 64 [10240/13374] loss=0.00016471542949147988\n",
      "[12min 41s] Epoch 64 [12800/13374] loss=0.00016405979200499133\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[12min 45s] Epoch 65 [2560/13374] loss=0.00011005187770933844\n",
      "[12min 47s] Epoch 65 [5120/13374] loss=0.00014220164048310834\n",
      "[12min 49s] Epoch 65 [7680/13374] loss=0.00014499106061217996\n",
      "[12min 51s] Epoch 65 [10240/13374] loss=0.00015426554327859776\n",
      "[12min 53s] Epoch 65 [12800/13374] loss=0.0001615169738943223\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[12min 56s] Epoch 66 [2560/13374] loss=0.000115605096652871\n",
      "[12min 58s] Epoch 66 [5120/13374] loss=0.0001280186523217708\n",
      "[13min 0s] Epoch 66 [7680/13374] loss=0.0001392223457514774\n",
      "[13min 2s] Epoch 66 [10240/13374] loss=0.00014460163838521113\n",
      "[13min 4s] Epoch 66 [12800/13374] loss=0.00015797060128534213\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[13min 8s] Epoch 67 [2560/13374] loss=0.0001457337726606056\n",
      "[13min 10s] Epoch 67 [5120/13374] loss=0.00014935541476006619\n",
      "[13min 12s] Epoch 67 [7680/13374] loss=0.00015431231828794505\n",
      "[13min 14s] Epoch 67 [10240/13374] loss=0.00016152456610143417\n",
      "[13min 16s] Epoch 67 [12800/13374] loss=0.00015805740666110068\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5632/6700 84.06%\n",
      "[13min 19s] Epoch 68 [2560/13374] loss=0.00013912576978327706\n",
      "[13min 21s] Epoch 68 [5120/13374] loss=0.0001431869954103604\n",
      "[13min 23s] Epoch 68 [7680/13374] loss=0.00013774622420896775\n",
      "[13min 25s] Epoch 68 [10240/13374] loss=0.0001480520810218877\n",
      "[13min 27s] Epoch 68 [12800/13374] loss=0.0001583616818243172\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[13min 31s] Epoch 69 [2560/13374] loss=0.0001311437110416591\n",
      "[13min 33s] Epoch 69 [5120/13374] loss=0.0001440768231987022\n",
      "[13min 35s] Epoch 69 [7680/13374] loss=0.00014533075785341982\n",
      "[13min 37s] Epoch 69 [10240/13374] loss=0.00014532973309542286\n",
      "[13min 39s] Epoch 69 [12800/13374] loss=0.00015598161509842613\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[13min 42s] Epoch 70 [2560/13374] loss=0.00010263301446684636\n",
      "[13min 44s] Epoch 70 [5120/13374] loss=0.00013130242077750154\n",
      "[13min 46s] Epoch 70 [7680/13374] loss=0.00014227483867822836\n",
      "[13min 48s] Epoch 70 [10240/13374] loss=0.00015068412612890826\n",
      "[13min 50s] Epoch 70 [12800/13374] loss=0.000159372290072497\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[13min 54s] Epoch 71 [2560/13374] loss=0.00012546483885671477\n",
      "[13min 56s] Epoch 71 [5120/13374] loss=0.00013938792635599385\n",
      "[13min 58s] Epoch 71 [7680/13374] loss=0.00014241745896773257\n",
      "[13min 59s] Epoch 71 [10240/13374] loss=0.0001557811977363599\n",
      "[14min 1s] Epoch 71 [12800/13374] loss=0.00015955807415593881\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[14min 5s] Epoch 72 [2560/13374] loss=0.0001323755113844527\n",
      "[14min 7s] Epoch 72 [5120/13374] loss=0.00015920934401947306\n",
      "[14min 9s] Epoch 72 [7680/13374] loss=0.00015902851797970166\n",
      "[14min 11s] Epoch 72 [10240/13374] loss=0.0001580427257977135\n",
      "[14min 13s] Epoch 72 [12800/13374] loss=0.00016066864809545224\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5611/6700 83.75%\n",
      "[14min 16s] Epoch 73 [2560/13374] loss=0.00011879021549248137\n",
      "[14min 18s] Epoch 73 [5120/13374] loss=0.00012614682455023286\n",
      "[14min 20s] Epoch 73 [7680/13374] loss=0.00013218408081835757\n",
      "[14min 22s] Epoch 73 [10240/13374] loss=0.00014744993532076479\n",
      "[14min 24s] Epoch 73 [12800/13374] loss=0.00015610240465321112\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5619/6700 83.87%\n",
      "[14min 28s] Epoch 74 [2560/13374] loss=0.0001388850054354407\n",
      "[14min 30s] Epoch 74 [5120/13374] loss=0.00013534792633436156\n",
      "[14min 32s] Epoch 74 [7680/13374] loss=0.00014864498759076621\n",
      "[14min 34s] Epoch 74 [10240/13374] loss=0.00015219045790217933\n",
      "[14min 36s] Epoch 74 [12800/13374] loss=0.0001554037481400883\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[14min 39s] Epoch 75 [2560/13374] loss=0.00015935629926389083\n",
      "[14min 41s] Epoch 75 [5120/13374] loss=0.00015234096026688347\n",
      "[14min 43s] Epoch 75 [7680/13374] loss=0.00014811661191439877\n",
      "[14min 45s] Epoch 75 [10240/13374] loss=0.0001511206937720999\n",
      "[14min 47s] Epoch 75 [12800/13374] loss=0.00016233116271905602\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[14min 51s] Epoch 76 [2560/13374] loss=0.0001340769602393266\n",
      "[14min 53s] Epoch 76 [5120/13374] loss=0.00013052848589722998\n",
      "[14min 55s] Epoch 76 [7680/13374] loss=0.00013632160723015357\n",
      "[14min 57s] Epoch 76 [10240/13374] loss=0.00013949124149803538\n",
      "[14min 59s] Epoch 76 [12800/13374] loss=0.00015608887799317017\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5616/6700 83.82%\n",
      "[15min 2s] Epoch 77 [2560/13374] loss=0.0001144499532529153\n",
      "[15min 4s] Epoch 77 [5120/13374] loss=0.0001230045239935862\n",
      "[15min 6s] Epoch 77 [7680/13374] loss=0.00013623122431454248\n",
      "[15min 8s] Epoch 77 [10240/13374] loss=0.00014503333368338646\n",
      "[15min 10s] Epoch 77 [12800/13374] loss=0.00015355011171777733\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5616/6700 83.82%\n",
      "[15min 14s] Epoch 78 [2560/13374] loss=0.00012438064441084863\n",
      "[15min 15s] Epoch 78 [5120/13374] loss=0.0001367136155749904\n",
      "[15min 17s] Epoch 78 [7680/13374] loss=0.00014063185954000802\n",
      "[15min 19s] Epoch 78 [10240/13374] loss=0.00014017355715623125\n",
      "[15min 21s] Epoch 78 [12800/13374] loss=0.00015268478280631827\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5629/6700 84.01%\n",
      "[15min 25s] Epoch 79 [2560/13374] loss=0.00012500128505053\n",
      "[15min 27s] Epoch 79 [5120/13374] loss=0.00013452146195049863\n",
      "[15min 29s] Epoch 79 [7680/13374] loss=0.00014174274656397756\n",
      "[15min 31s] Epoch 79 [10240/13374] loss=0.00014025160717210384\n",
      "[15min 33s] Epoch 79 [12800/13374] loss=0.00015233788850309792\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[15min 36s] Epoch 80 [2560/13374] loss=0.00014129950432106853\n",
      "[15min 38s] Epoch 80 [5120/13374] loss=0.00013410944156930782\n",
      "[15min 40s] Epoch 80 [7680/13374] loss=0.00013924223336895616\n",
      "[15min 42s] Epoch 80 [10240/13374] loss=0.00014281546045822325\n",
      "[15min 44s] Epoch 80 [12800/13374] loss=0.0001525222598866094\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[15min 48s] Epoch 81 [2560/13374] loss=0.0001187617759569548\n",
      "[15min 50s] Epoch 81 [5120/13374] loss=0.00012005797871097456\n",
      "[15min 52s] Epoch 81 [7680/13374] loss=0.00013804085222849\n",
      "[15min 54s] Epoch 81 [10240/13374] loss=0.0001507557783043012\n",
      "[15min 56s] Epoch 81 [12800/13374] loss=0.00015417907023220322\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[15min 59s] Epoch 82 [2560/13374] loss=0.00012122148400521837\n",
      "[16min 1s] Epoch 82 [5120/13374] loss=0.00012184791903564473\n",
      "[16min 3s] Epoch 82 [7680/13374] loss=0.00012958374900335913\n",
      "[16min 5s] Epoch 82 [10240/13374] loss=0.0001433843414815783\n",
      "[16min 7s] Epoch 82 [12800/13374] loss=0.00015337562530476134\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5621/6700 83.90%\n",
      "[16min 11s] Epoch 83 [2560/13374] loss=0.00013055148883722723\n",
      "[16min 13s] Epoch 83 [5120/13374] loss=0.0001443422781449044\n",
      "[16min 15s] Epoch 83 [7680/13374] loss=0.00013978220764935637\n",
      "[16min 17s] Epoch 83 [10240/13374] loss=0.00014466108623309993\n",
      "[16min 18s] Epoch 83 [12800/13374] loss=0.00015062801394378768\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5633/6700 84.07%\n",
      "[16min 22s] Epoch 84 [2560/13374] loss=0.0001097705819120165\n",
      "[16min 24s] Epoch 84 [5120/13374] loss=0.00012142937412136234\n",
      "[16min 26s] Epoch 84 [7680/13374] loss=0.00013402466580979914\n",
      "[16min 28s] Epoch 84 [10240/13374] loss=0.00014675200259262056\n",
      "[16min 30s] Epoch 84 [12800/13374] loss=0.00015173594820225845\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5598/6700 83.55%\n",
      "[16min 34s] Epoch 85 [2560/13374] loss=0.00011850503360619769\n",
      "[16min 36s] Epoch 85 [5120/13374] loss=0.00013200618996052073\n",
      "[16min 38s] Epoch 85 [7680/13374] loss=0.00014144347805995495\n",
      "[16min 39s] Epoch 85 [10240/13374] loss=0.00014525529768434354\n",
      "[16min 41s] Epoch 85 [12800/13374] loss=0.00015126680635148658\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[16min 45s] Epoch 86 [2560/13374] loss=0.00013649985194206238\n",
      "[16min 47s] Epoch 86 [5120/13374] loss=0.00012408804068400058\n",
      "[16min 49s] Epoch 86 [7680/13374] loss=0.00013548859302924636\n",
      "[16min 51s] Epoch 86 [10240/13374] loss=0.00014718317233928247\n",
      "[16min 53s] Epoch 86 [12800/13374] loss=0.00015277394020813518\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5631/6700 84.04%\n",
      "[16min 57s] Epoch 87 [2560/13374] loss=0.00013511344441212713\n",
      "[16min 58s] Epoch 87 [5120/13374] loss=0.00012933735197293573\n",
      "[17min 0s] Epoch 87 [7680/13374] loss=0.00013685946735980298\n",
      "[17min 2s] Epoch 87 [10240/13374] loss=0.00015051429636514513\n",
      "[17min 4s] Epoch 87 [12800/13374] loss=0.00015334598647314123\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[17min 8s] Epoch 88 [2560/13374] loss=0.00012406648020260035\n",
      "[17min 10s] Epoch 88 [5120/13374] loss=0.00014702160224260296\n",
      "[17min 12s] Epoch 88 [7680/13374] loss=0.00014533095139389237\n",
      "[17min 14s] Epoch 88 [10240/13374] loss=0.00014444113767240197\n",
      "[17min 16s] Epoch 88 [12800/13374] loss=0.00014740032944246195\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5612/6700 83.76%\n",
      "[17min 19s] Epoch 89 [2560/13374] loss=0.00011377950650057756\n",
      "[17min 21s] Epoch 89 [5120/13374] loss=0.00012846860918216407\n",
      "[17min 23s] Epoch 89 [7680/13374] loss=0.00013318736625175613\n",
      "[17min 25s] Epoch 89 [10240/13374] loss=0.0001402806788973976\n",
      "[17min 27s] Epoch 89 [12800/13374] loss=0.0001499769125075545\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5620/6700 83.88%\n",
      "[17min 31s] Epoch 90 [2560/13374] loss=0.00011903373815584928\n",
      "[17min 33s] Epoch 90 [5120/13374] loss=0.00012991289568162757\n",
      "[17min 35s] Epoch 90 [7680/13374] loss=0.0001310818786199282\n",
      "[17min 37s] Epoch 90 [10240/13374] loss=0.0001393749168528302\n",
      "[17min 39s] Epoch 90 [12800/13374] loss=0.0001482514036615612\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[17min 42s] Epoch 91 [2560/13374] loss=0.00012695674813585357\n",
      "[17min 44s] Epoch 91 [5120/13374] loss=0.00013438518762995955\n",
      "[17min 46s] Epoch 91 [7680/13374] loss=0.00014065900783558996\n",
      "[17min 48s] Epoch 91 [10240/13374] loss=0.00014839544401183956\n",
      "[17min 50s] Epoch 91 [12800/13374] loss=0.00015136059460928663\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[17min 54s] Epoch 92 [2560/13374] loss=0.00013028728135395796\n",
      "[17min 56s] Epoch 92 [5120/13374] loss=0.00012426775974745398\n",
      "[17min 58s] Epoch 92 [7680/13374] loss=0.00013784502128449578\n",
      "[18min 0s] Epoch 92 [10240/13374] loss=0.00013523717989301077\n",
      "[18min 2s] Epoch 92 [12800/13374] loss=0.00014696677812025882\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5622/6700 83.91%\n",
      "[18min 5s] Epoch 93 [2560/13374] loss=0.0001348498321021907\n",
      "[18min 7s] Epoch 93 [5120/13374] loss=0.00013211600125941913\n",
      "[18min 9s] Epoch 93 [7680/13374] loss=0.00013895449834914567\n",
      "[18min 11s] Epoch 93 [10240/13374] loss=0.00014123976088740165\n",
      "[18min 13s] Epoch 93 [12800/13374] loss=0.00014685896356240847\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5628/6700 84.00%\n",
      "[18min 17s] Epoch 94 [2560/13374] loss=0.00011584029780351557\n",
      "[18min 19s] Epoch 94 [5120/13374] loss=0.00012563490236061625\n",
      "[18min 21s] Epoch 94 [7680/13374] loss=0.0001428640772549746\n",
      "[18min 23s] Epoch 94 [10240/13374] loss=0.00015434131146321305\n",
      "[18min 25s] Epoch 94 [12800/13374] loss=0.00014994678800576367\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[18min 28s] Epoch 95 [2560/13374] loss=0.00011588758643483743\n",
      "[18min 30s] Epoch 95 [5120/13374] loss=0.00013805813614453655\n",
      "[18min 32s] Epoch 95 [7680/13374] loss=0.00014615730566826338\n",
      "[18min 34s] Epoch 95 [10240/13374] loss=0.00014828444400336594\n",
      "[18min 36s] Epoch 95 [12800/13374] loss=0.00014987932911026292\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5614/6700 83.79%\n",
      "[18min 40s] Epoch 96 [2560/13374] loss=0.00011034630369977094\n",
      "[18min 42s] Epoch 96 [5120/13374] loss=0.0001254687798791565\n",
      "[18min 44s] Epoch 96 [7680/13374] loss=0.00013128189433094425\n",
      "[18min 46s] Epoch 96 [10240/13374] loss=0.0001387107873597415\n",
      "[18min 48s] Epoch 96 [12800/13374] loss=0.0001461269668652676\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[18min 52s] Epoch 97 [2560/13374] loss=0.0001349131613096688\n",
      "[18min 54s] Epoch 97 [5120/13374] loss=0.00014072275480430106\n",
      "[18min 56s] Epoch 97 [7680/13374] loss=0.00014947503271590297\n",
      "[30min 41s] Epoch 97 [10240/13374] loss=0.0001497450133683742\n",
      "[30min 43s] Epoch 97 [12800/13374] loss=0.00015023023312096484\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5591/6700 83.45%\n",
      "[30min 47s] Epoch 98 [2560/13374] loss=0.00010461835518071894\n",
      "[30min 50s] Epoch 98 [5120/13374] loss=0.00012207417294121114\n",
      "[30min 52s] Epoch 98 [7680/13374] loss=0.00013591610816850638\n",
      "[30min 54s] Epoch 98 [10240/13374] loss=0.00014654295728178113\n",
      "[30min 56s] Epoch 98 [12800/13374] loss=0.00014601548456994351\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[31min 0s] Epoch 99 [2560/13374] loss=0.00012085143134754617\n",
      "[31min 2s] Epoch 99 [5120/13374] loss=0.00013280441216920736\n",
      "[31min 3s] Epoch 99 [7680/13374] loss=0.00013986867391698374\n",
      "[31min 5s] Epoch 99 [10240/13374] loss=0.00014336458771140314\n",
      "[31min 7s] Epoch 99 [12800/13374] loss=0.0001451119186822325\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[31min 11s] Epoch 100 [2560/13374] loss=0.0001406415132805705\n",
      "[31min 13s] Epoch 100 [5120/13374] loss=0.00012996229634154588\n",
      "[31min 15s] Epoch 100 [7680/13374] loss=0.000135468188818777\n",
      "[31min 17s] Epoch 100 [10240/13374] loss=0.0001424965994374361\n",
      "[31min 19s] Epoch 100 [12800/13374] loss=0.00014370402568602002\n",
      "evaluating trained model ...\n",
      "Test set: Accuracy 5623/6700 83.93%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import gzip  # 用于读取压缩文件\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# 一些超参数\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256  # 一次训练的样本数，为256个名字\n",
    "N_LAYER = 2  # RNN的层数\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128  # ASCII码一共有128个字符\n",
    "USE_GPU = False\n",
    "\n",
    "\n",
    "# 构造数据集\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = '../Data/names_train.csv.gz' if is_train_set else '../Data/names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:  # rt表示以只读模式打开文件，并将文件内容解析为文本形式\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)  # rows是一个列表，每个元素是一个名字和国家名组成的列表\n",
    "        self.names = [row[0] for row in rows]  # 一个很长的列表，每个元素是一个名字，字符串，长度不一，需要转化为数字\n",
    "        self.len = len(self.names)  # 训练集：13374  测试集：6700\n",
    "        self.countries = [row[1] for row in rows]  # 一个很长的列表，每个元素是一个国家名，字符串，需要编码成数字\n",
    "        # 下面两行的作用其实就是把国家名编码成数字，因为后面要用到交叉熵损失函数\n",
    "        self.country_list = list(sorted(set(self.countries)))  # 列表，按字母表顺序排序，去重后有18个国家名\n",
    "        self.country_dict = self.getCountryDict()  # 字典，key是国家名，value是country_list的国家名对应的索引(0-17)\n",
    "        self.country_num = len(self.country_list)  # 18\n",
    "\n",
    "    # 根据样本的索引返回姓名和国家名对应的索引，可以理解为(特征,标签)，但这里的特征是姓名，后面还需要转化为数字，标签是国家名对应的索引\n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "\n",
    "    # 返回样本数量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    # 返回一个key为国家名和value为索引的字典\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()  # 空字典\n",
    "        for idx, country_name in enumerate(self.country_list):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "\n",
    "    # 根据索引(标签值)返回对应的国家名\n",
    "    def idx2country(self, index):\n",
    "        return self.country_list[index]\n",
    "\n",
    "    # 返回国家名(标签类别)的个数，18\n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num\n",
    "\n",
    "\n",
    "# 实例化数据集\n",
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainset.getCountriesNum()  # 18个国家名，即18个类别\n",
    "\n",
    "\n",
    "# 设计神经网络模型\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size  # 隐含层的大小，100\n",
    "        self.n_layers = n_layers  # RNN的层数，2\n",
    "        self.n_directions = 2 if bidirectional else 1  # 是否使用双向RNN\n",
    "\n",
    "        # 词嵌入层：input_size是输入的特征数(即不同词语的个数)，即128;embedding_size是词嵌入的维度(即将词语映射到的向量的维度)，这里让它等于了隐含层的大小，即100\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        # GRU层：input_size是输入的特征数(这里是embedding_size,其大小等于hidden_size)，即100;hidden_size是隐含层的大小，即100;n_layers是RNN的层数，2;bidirectional是是否使用双向RNN\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "        # 全连接层：hidden_size是隐含层的大小，即100;output_size是输出的特征数(即不同类别的个数)，即18\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        # 初始化隐含层，形状为(n_layers * num_directions, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape:B X S -> S X B\n",
    "        input = input.t()  # 转置，变成(seq_len,batch_size)\n",
    "        batch_size = input.size(1)  # 256，一次训练的样本数，为256个名字，即batch_size\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        # 1、嵌入层处理，input:(seq_len,batch_size) -> embedding:(seq_len,batch_size,embedding_size)\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        gru_input = pack_padded_sequence(embedding, seq_lengths)\n",
    "\n",
    "        # output:(*, hidden_size * num_directions)，*表示输入的形状(seq_len,batch_size)\n",
    "        # hidden:(num_layers * num_directions, batch, hidden_size)\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]],\n",
    "                                   dim=1)  # hidden[-1]的形状是(1,256,100)，hidden[-2]的形状是(1,256,100)，拼接后的形状是(1,256,200)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]  # (1,256,100)\n",
    "        fc_output = self.fc(hidden_cat)  # 返回的是(1,256,18)\n",
    "        return fc_output\n",
    "\n",
    "\n",
    "# 下面该函数属于数据准备阶段的延续部分，因为神经网络只能处理数字，不能处理字符串，所以还需要把姓名转换成数字\n",
    "def make_tensors(names, countries):\n",
    "    # 传入的names是一个列表，每个元素是一个姓名字符串，countries也是一个列表，每个元素是一个整数\n",
    "    sequences_and_lengths = [name2list(name) for name in\n",
    "                             names]  # 返回的是一个列表，每个元素是一个元组，元组的第一个元素是姓名字符串转换成的数字列表，第二个元素是姓名字符串的长度\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths]  # 返回的是一个列表，每个元素是姓名字符串转换成的数字列表\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])  # 返回的是一个列表，每个元素是姓名字符串的长度\n",
    "    countries = countries.long()  # PyTorch 中，张量的默认数据类型是浮点型 (float)，这里转换成整型，可以避免浮点数比较时的精度误差，从而提高模型的训练效果\n",
    "\n",
    "    # make tensor of name, (Batch_size,Seq_len) 实现填充0的功能\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    # perm_idx是排序后的数据在原数据中的索引,seq_tensor是排序后的数据,seq_lengths是排序后的数据的长度,countries是排序后的国家\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "\n",
    "    return create_tensor(seq_tensor), create_tensor(seq_lengths), create_tensor(countries)\n",
    "\n",
    "\n",
    "# 把名字转换成ASCII码，返回ASCII码值列表和名字的长度\n",
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    "\n",
    "\n",
    "# 是否把数据放到GPU上\n",
    "def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{timeSince(start)}] Epoch {epoch} ', end='')  # end=''表示不换行\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')  # 打印每个样本的平均损失\n",
    "\n",
    "    return total_loss  # 返回的是所有样本的损失，我们并没有用上它\n",
    "\n",
    "\n",
    "# 测试模型\n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print('evaluating trained model ...')\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]  # 返回每一行中最大值的那个元素的索引，且keepdim=True，表示保持输出的二维特性\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # 计算正确的个数\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "\n",
    "    return correct / total  # 返回的是准确率，0.几几的格式，用来画图\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)  # math.floor()向下取整\n",
    "    s -= m * 60\n",
    "    return '%dmin %ds' % (m, s)  # 多少分钟多少秒\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        classifier.to(device)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    start = time.time()\n",
    "    print('Training for %d epochs...' % N_EPOCHS)\n",
    "    acc_list = []\n",
    "    # 在每个epoch中，训练完一次就测试一次\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    # 绘制在测试集上的准确率\n",
    "    epoch = np.arange(1, len(acc_list) + 1)\n",
    "    acc_list = np.array(acc_list)\n",
    "    plt.plot(epoch, acc_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在机器学习中，训练数据和测试数据是两个独立的数据集，用于训练和评估模型的性能。在训练模型时，通常会对训练数据进行随机打乱，以确保每个样本都有机会被用于训练，减少模型对数据的过拟合，提高泛化能力。但是，对于测试数据，我们希望评估模型在未见过的数据上的性能，因此不应该对测试数据进行打乱，以确保模型在真实场景中的性能得到准确评估。\n",
    "\n",
    "因此，在上述代码中，我们对训练数据集进行shuffle=True的打乱操作，以便每个batch的数据是随机的，有助于模型更好地学习到数据的分布特征；而对于测试数据集，我们设置shuffle=False，以确保测试集中的数据顺序不会影响模型的评估结果，避免测试结果受到无意义的影响。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}